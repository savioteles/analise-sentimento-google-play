{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "46b16818-f7f1-42cc-8488-281439eb30be",
          "showTitle": false,
          "title": ""
        },
        "id": "_p3ekV5wDyHy"
      },
      "source": [
        "# Introdução ao Big Data com Apache Spark\n",
        "\n",
        "Este tutorial tem como objetivo facilitar o entendimento do conceito de Big Data utilizando o Apache Spark.\n",
        "    \n",
        "# Arquitetura do Apache Spark\n",
        "\n",
        "Antes de partir para o código, vamos ver uma visão geral da arquitetura do Apache Spark. Esta arquitetura permite que você possa processar seus códigos em várias máquinas como se fosse uma só através da arquitetura master-worker, onde existe um `driver` ou nó master no cluster, acompanhado pelos nós `worker`. O master envia o trabalho para os workers com instruções para carregar os dados da memória ou do disco.\n",
        "\n",
        "O diagrama abaixo apresenta um exemplo de um cluster com Apache Spark, onde basicamente existe um nó Driver que comunica com os nós executors. Cada um destes nós executors tem slots que são logicamente como núcleos de processsamento.\n",
        "\n",
        "![spark-architecture](https://intellipaat.com/mediaFiles/2017/02/Spark-Arch.jpg)\n",
        "\n",
        "\n",
        "\n",
        "*   https://www.johnsnowlabs.com/unlocking-the-power-of-sentiment-analysis-with-deep-learning/\n",
        "*   https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/sentiment_dl/index.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c330e841-e98f-4ff8-8da8-6f47e221280c",
          "showTitle": false,
          "title": ""
        },
        "id": "_3rB_1gVDyIN"
      },
      "source": [
        "# SparkSession - Um ponto de entrada unificado no Apache Spark 2.0\n",
        "\n",
        "No Spark 2.0, foi introduzido o [SparkSession](https://spark.apache.org/docs/preview/api/python/pyspark.sql.html#pyspark.sql.SparkSession), um novo ponto de entrada que agrega o SparkContext, SQLContext, StreamingContext, and HiveContext. SparkSession possui muitas funcionalidades e nesse notebook vamos apresentar as mais importantes para ilustrar o acesso às funcionalidades do Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark NLP\n",
        "\n",
        "Neste tutorial vamos fazer uso de uma técnica denominada Processamento de Linguagem Natural, mais conhecido pelo termo em inglês Natural Language Processing (NLP). O processamento de linguagem natural usa machine learning para revelar a estrutura e o significado do texto. Com aplicativos de processamento de linguagem natural, as organizações podem analisar textos e extrair informações sobre pessoas, lugares e eventos para entender melhor as opiniões em mídias sociais e conversas de clientes.\n",
        "\n",
        "Spark NLP é uma biblioteca de processamento de linguagem natural de última geração que foi construída sobre o Apache Spark. Ele fornece bibliotecas de NLP simples, eficientes e precisas para pipelines de aprendizado de máquina que podem ser implantadas facilmente em um ambiente distribuído. Ele oferece suporte a quase todas as tarefas e módulos de NLP que podem ser usados sem problemas em um cluster Spark.\n",
        "\n",
        "<img src=\"https://www.johnsnowlabs.com/wp-content/uploads/2023/05/23_05_2023.jpg\"  width=\"90%\" height=\"70%\">\n"
      ],
      "metadata": {
        "id": "Uhwz5rgvpF00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial com Spark NLP\n",
        "Neste tutorial iremos usar a análise de sentimentos para localizar e rotular campos em reviews de usuários do Google Play para entender melhor as opiniões dos clientes e encontrar insights sobre o produto e a experiência do usuário."
      ],
      "metadata": {
        "id": "lC6YzxkqvwPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Google Colab é talvez a maneira mais fácil de começar a usar o spark-nlp. Ele não requer nenhuma instalação ou configuração além de ter uma conta do Google.\n",
        "\n",
        "Execute o código a seguir no notebook do Google Colab e comece a usar o Spark NLP imediatamente. O Spark NLP é compatível com Python 3.7.x e superior, dependendo da versão principal do PySpark."
      ],
      "metadata": {
        "id": "J--PRx4Kv2R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is only to setup PySpark and Spark NLP on Colab\n",
        "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "metadata": {
        "id": "QPM--fsfPFZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para iniciar um cluster Spark com suporte a NLP, basta importar a biblioteca e iniciar a sessão executando o comando abaixo. O parâmetro gpu=True indica para o Spark NLP que irá usar a GPU no processamento dos textos, caso tenha alguma disponível."
      ],
      "metadata": {
        "id": "bDEiVOW4xEWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start(gpu=True)\n",
        "\n",
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ],
      "metadata": {
        "id": "kFghABwTPGgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise de Sentimento com Spark NLP\n",
        "\n",
        "Neste tutorial iremos utilizar modelos de Deep Learning (DL) com Large Language Models (LLM) para análise de sentimentos.\n",
        "\n",
        "Os modelos de DL para análise de sentimentos podem aprender representações de texto automaticamente sem a necessidade de engenharia de feature. Esses modelos podem capturar padrões mais complexos nos dados e podem ter um desempenho melhor em tarefas com mais nuances, como detecção de sarcasmo ou reconhecimento de emoções. No entanto, eles podem exigir conjuntos de dados maiores para treinamento e podem ser computacionalmente caros, exigindo recursos de computação de alto desempenho (GPUs)."
      ],
      "metadata": {
        "id": "e55rXRW-TeJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salvando o modelo do HuggingFace\n",
        "\n",
        "Em muitos casos, você pode obter resultados de alta qualidade com modelos de aprendizado de máquina que foram treinados anteriormente em grandes conjuntos de dados de texto. Muitos desses modelos pré-treinados estão disponíveis em código aberto e são de uso gratuito. A HuggingFace é uma excelente fonte desses modelos, e sua biblioteca Transformers é uma ferramenta fácil de usar para aplicar os modelos e também adaptá-los aos seus próprios dados.\n",
        "\n",
        "Neste artigo iremos mostrar como utilizar modelos do HuggingFace pré-treinados para carregar no Spark."
      ],
      "metadata": {
        "id": "Rsue7XlaTYq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O primeiro passo é instalar e importar as bibliotecas transformers e tensorflow:"
      ],
      "metadata": {
        "id": "deOlNyZ944C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.25.1 tensorflow==2.11.0"
      ],
      "metadata": {
        "id": "JvJMC-19TWW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "w55FzEzvlasZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iremos utilizar um modelo (https://huggingface.co/ramonmedeiro1/bertimbau-products-reviews-pt-br) treinado a partir do Bertimbau da Neuralmind (https://huggingface.co/neuralmind/bert-base-portuguese-cased) em um dataset chamado B2W-Reviews01. Este é um corpus aberto de reviews de produtos contendo mais de 130 mil avaliações de clientes de comércio eletrônico, coletadas no site da Americanas.com."
      ],
      "metadata": {
        "id": "ujtmIENY5LCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"ramonmedeiro1/bertimbau-products-reviews-pt-br\""
      ],
      "metadata": {
        "id": "zcNBCsQgj1xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para carregar o modelo iremos utilizar a função `from_pretrained`:"
      ],
      "metadata": {
        "id": "acBA6-bl_axK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "mQULGOax_d5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O HuggingFace vem com uma função `save_pretrained` para modelos baseados no TensorFlow. Usaremos isso para salvar o vocabulário do modelo."
      ],
      "metadata": {
        "id": "IlVsrg3YAptR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained('./{}_tokenizer/'.format(MODEL_NAME))"
      ],
      "metadata": {
        "id": "JF3U1yjuBDuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iremos carregar o modelo usando o `TFBertForSequenceClassification` que é usado para classificação de sentenças:"
      ],
      "metadata": {
        "id": "0MV4FK9wFO6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "except:\n",
        "  model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME, from_pt=True)"
      ],
      "metadata": {
        "id": "ycqxR5FmTYBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O trecho de código abaixo salva o modelo completo que será usado, posteriormente, para análise de sentimento:"
      ],
      "metadata": {
        "id": "NBR5lgnPGf1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define TF Signature\n",
        "@tf.function(\n",
        "  input_signature=[\n",
        "      {\n",
        "          \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n",
        "          \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n",
        "          \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n",
        "      }\n",
        "  ]\n",
        ")\n",
        "def serving_fn(input):\n",
        "    return model(input)\n",
        "\n",
        "model.save_pretrained(\"./{}\".format(MODEL_NAME), saved_model=True, signatures={\"serving_default\": serving_fn})"
      ],
      "metadata": {
        "id": "9X3lWPLrlg_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pasta de assets está vazia e iremos copiar o vocabulário para esta pasta. Agora o modelo estará completo:"
      ],
      "metadata": {
        "id": "_8LdnpDpH6VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asset_path = '{}/saved_model/1/assets'.format(MODEL_NAME)\n",
        "\n",
        "!cp {MODEL_NAME}_tokenizer/vocab.txt {asset_path}"
      ],
      "metadata": {
        "id": "whaLGrYWT6yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carrega o dicionário label2id com os labels de sentimentos dos reviews:"
      ],
      "metadata": {
        "id": "C75kLQf-IHDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = model.config.label2id"
      ],
      "metadata": {
        "id": "V7Q5z36lIOLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordena o dicionário baseado no id"
      ],
      "metadata": {
        "id": "pi_0A0XEIO1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = sorted(labels, key=labels.get)"
      ],
      "metadata": {
        "id": "Oc3Jw4j_IdN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salva o dicionário com os labels de sentimentos na pasta de assets:"
      ],
      "metadata": {
        "id": "r_ogwVbPIyMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(asset_path+'/labels.txt', 'w') as f:\n",
        "    f.write('\\n'.join(labels))"
      ],
      "metadata": {
        "id": "ibUtP-JET7be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coletando os reviews"
      ],
      "metadata": {
        "id": "n-pkd6cOj2S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instala a biblioteca Google play scraper: https://github.com/JoMingyu/google-play-scraper\n",
        "!pip install google_play_scraper"
      ],
      "metadata": {
        "id": "Bgcj78Zzj9P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google_play_scraper import Sort, reviews, app\n",
        "rvs, _ = reviews(\n",
        "                'com.amazon.mShop.android.shopping',\n",
        "                lang='pt',\n",
        "                country='br',\n",
        "                sort='most_relevant',\n",
        "                count= 20\n",
        "            )"
      ],
      "metadata": {
        "id": "NvGC568okPgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "app_reviews_df = pd.DataFrame(rvs)"
      ],
      "metadata": {
        "id": "S6JeTEqok_T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df = spark.createDataFrame(app_reviews_df[['reviewId', 'content', 'score', 'at']])"
      ],
      "metadata": {
        "id": "7cqDdvS4lOCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando o Pipeline de NLP\n",
        "O Spark NLP processa os dados usando Pipelines, estrutura que contém todas as etapas a serem executadas nos dados de entrada.\n",
        "\n",
        "Cada etapa contém um `Annotator` que executa uma tarefa específica, como tokenização, normalização e análise de dependência. Cada `Annotator` tem anotação(ões) de entrada e produz nova anotação. Um `Annotator` pega um documento de texto de entrada e produz um documento de saída com metadados adicionais, que podem ser usados para processamento ou análise posterior."
      ],
      "metadata": {
        "id": "N2OGarXkVdyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *"
      ],
      "metadata": {
        "id": "N6k8X6rW8Qnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para passar pelo pipeline de NLP, precisamos anotar os dados brutos. O `DocumentAssembler` prepara os textos de entrada em um formato que pode ser processado pelo Spark NLP. Esse é o ponto de entrada para todos os pipelines do Spark NLP."
      ],
      "metadata": {
        "id": "kjWnAeG_sdqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol('content') \\\n",
        "    .setOutputCol('document')"
      ],
      "metadata": {
        "id": "B4mx1pgvTeJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classe Tokenizer irá gerar os tokens a partir do documento de entrada:"
      ],
      "metadata": {
        "id": "wtIouva91NBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(['document']) \\\n",
        "    .setOutputCol('token')"
      ],
      "metadata": {
        "id": "8BBaw6zPTeJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vamos usar a função `loadSavedModel` da classe `BertForSequenceClassification`, que nos permite carregar o modelo do TensorFlow no formato SavedModel\n",
        "* A maioria dos parâmetros pode ser definida posteriormente, quando você estiver carregando esse modelo no `BertForSequenceClassification` em tempo de execução, como `setMaxSentenceLength`, portanto, não se preocupe com o que está definindo agora.\n",
        "* `loadSavedModel` aceita dois parâmetros:\n",
        "  * o primeiro é o caminho para o TF SavedModel.\n",
        "  * o segundo é a SparkSession, que é a variável Spark que iniciamos anteriormente por meio de `sparknlp.start()`"
      ],
      "metadata": {
        "id": "EQ3-oy8f5bK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequenceClassifier = BertForSequenceClassification.loadSavedModel(\n",
        "     '{}/saved_model/1'.format(MODEL_NAME),\n",
        "     spark\n",
        " )\\\n",
        "  .setInputCols([\"document\",'token'])\\\n",
        "  .setOutputCol(\"class\")\\\n",
        "  .setMaxSentenceLength(128)"
      ],
      "metadata": {
        "id": "CgnkMPkrVgJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# introduzido no spark-nlp==3.4.0\n",
        "sequenceClassifier.getClasses()"
      ],
      "metadata": {
        "id": "NNpqFzgRWIDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora criamos o Pipeline com os seguintes estágios:\n",
        "\n",
        "\n",
        "1.   `Document Assembler`: recebe o texto de entrada e gera um objeto  `document` que contém os dados e metadados para o modelo de IA.\n",
        "2.   `Tokenizer`: transforma o texto em tokens (objeto `token`) que serão utilizados pelo Transformer na análise de sentimento.\n",
        "3. `SequenceClassifier`: recebe os objetos `document` e `token` e gera como saída o resultado final da análise de sentimento.\n",
        "\n"
      ],
      "metadata": {
        "id": "eFzZJc9E1fVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    sequenceClassifier\n",
        "])"
      ],
      "metadata": {
        "id": "wTQ5f8spTeJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fazendo a análise de sentimento dos reviews"
      ],
      "metadata": {
        "id": "naM8b19M4aOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo executa a inferência e determina o sentimento de cada linha do Dataframe:"
      ],
      "metadata": {
        "id": "1lWdPDC96GnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipeline.fit(reviews_df).transform(reviews_df)"
      ],
      "metadata": {
        "id": "Nfz_eeXclyS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O resultado da análise de sentimento é colocada em um vetor e temos que transformar em uma String:"
      ],
      "metadata": {
        "id": "NXIzzQ4P6LJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.withColumn(\"result\", result[\"class.result\"].getItem(0))"
      ],
      "metadata": {
        "id": "kAHutJlIxF-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui fazemos um tratamento para poder comparar o resultado do modelo com o review do usuário. Por isso, iremos ter três classes de resultados:\n",
        "\n",
        "* **0.0**: Negativo e Muito Negativo\n",
        "* **1.0**: Neutro\n",
        "* **2.0**: Positivo e Muito Positivo"
      ],
      "metadata": {
        "id": "ylH3Z3Uv6T6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col, lit\n",
        "result = result.withColumn(\"prediction\",\n",
        "       when((col('result') == lit('Negativo')) | (col('result') == lit('Muito Negativo')), 0.0)\n",
        "      .when(col('result') == lit('Neutro'), 1.0)\n",
        "      .otherwise(2.0))"
      ],
      "metadata": {
        "id": "mAtLzniUw9KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui fazemos um tratamento para poder comparar o resultado do modelo com o review do usuário. O score do usuário no Google Play vai de 1 a 5 e iremos mapear em três classes de resultados:\n",
        "\n",
        "* **0.0**: Score 1 e 2\n",
        "* **1.0**: Score 3\n",
        "* **2.0**: Score 4 e 5"
      ],
      "metadata": {
        "id": "wqO-CMXG6sGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.withColumn(\"label\",\n",
        "       when((col('score') == 1) | (col('score') == 2), 0.0)\n",
        "      .when(col('score') == 3, 1.0)\n",
        "      .otherwise(2.0))"
      ],
      "metadata": {
        "id": "0l76GnlHw95F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('content', 'prediction', 'label', 'score', 'result').filter(\"label != prediction\").show(truncate=False)"
      ],
      "metadata": {
        "id": "mjnnW8wkxQCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predizendo reviews"
      ],
      "metadata": {
        "id": "hi8VKoMHj5GI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui fazemos a análise de sentimento em lote a partir do arquivo csv que baixamos dos reviews do Google Play."
      ],
      "metadata": {
        "id": "8y8Efe8-69Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1oquPsLdKqAUphOLXPbQ2YAZ15xELtXUu"
      ],
      "metadata": {
        "id": "olnLDUi8TeJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta linha abaixo lê o arquivo csv com o Apache Spark:"
      ],
      "metadata": {
        "id": "WFdORbHI7FBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = spark.read.csv(\"reviews.csv\", header=True, quote=\"\\\"\", escape=\"\\\"\", multiLine=True)"
      ],
      "metadata": {
        "id": "nankWWmpTeJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iremos utilizar na análise de sentimento apenas as colunas \"reviewId\", \"content\", \"score\", \"sentiment\". Por isso, utilizamos a função `select` do Spark para selecionar apenas estas colunas:"
      ],
      "metadata": {
        "id": "jFZXH9qc7JfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_dataset.select(\"reviewId\", \"content\", \"score\", \"sentiment\")"
      ],
      "metadata": {
        "id": "RaspzYQgDN5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui fazemos a análise de sentimento de cada texto do csv:"
      ],
      "metadata": {
        "id": "wJCKX-_W7U2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipeline.fit(test_dataset).transform(test_dataset)"
      ],
      "metadata": {
        "id": "IGOpr6Sr5cca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O resultado da análise de sentimento é colocada em um vetor e temos que transformar em uma String:"
      ],
      "metadata": {
        "id": "VQXzPRWg7cWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.withColumn(\"result\", result[\"class.result\"].getItem(0))"
      ],
      "metadata": {
        "id": "bXJO8oMTNHWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nas duas células abaixo fazemos a transformação das colunas de resultado do modelo e do review do usuário para saber a acurácia do modelo:"
      ],
      "metadata": {
        "id": "19Tt-Edk7dt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col, lit\n",
        "result = result.withColumn(\"prediction\",\n",
        "       when((col('result') == lit('Negativo')) | (col('result') == lit('Muito Negativo')), 0.0)\n",
        "      .when(col('result') == lit('Neutro'), 1.0)\n",
        "      .otherwise(2.0))"
      ],
      "metadata": {
        "id": "jLVOBAVn0Wil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.withColumn(\"label\",\n",
        "       when((col('score') == 1) | (col('score') == 2), 0.0)\n",
        "      .when(col('score') == 3, 1.0)\n",
        "      .otherwise(2.0))"
      ],
      "metadata": {
        "id": "dxIHs5rO2bGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classe `MulticlassClassificationEvaluator` do Apache Spark ML faz o cálculo automático da acurácia do modelo utilizando os rótulos esperados (reviews do usuário) e o resultado do modelo (análise de sentimento)."
      ],
      "metadata": {
        "id": "L_v_brep7omu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select (prediction, true label) and compute test error\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(result)\n",
        "print(f\"Accuray = {accuracy}\")"
      ],
      "metadata": {
        "id": "dlkgOjRA3KQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "Introdução ao Big Data com Apache Spark na Plataforma Databricks",
      "widgets": {}
    },
    "name": "Introdução ao Big Data com Apache Spark na Plataforma Databricks",
    "notebookId": 3241280870852609,
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}